{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vioE_ucBJYd"
      },
      "source": [
        "# Approximate betweenness centrality using neural networks\n",
        "Here we start to approximate the betweennesss centrality using neural networks over a peer-2-peer network Gnutella. Gnutella is a set of datasets consisting of 9 networks ranging from 6,300 to 63,000 nodes. Our goal is to train a neural network on the smallest Gnurella graph and evaluate it on a much larger graph. We will guide you through this step by step.\n",
        "\n",
        "You can find Gnutella datasets at http://snap.stanford.edu/data/index.html. We will use p2p-Gnutella08 for training and p2p-Gnutella04 for testing.\n",
        "\n",
        "Note:\n",
        "1. Copy this notebook to your Google drive in order to execute it.\n",
        "2. Make sure to upload the data files in HW4 to your google drive and to modify their corresponding directories in the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW0BOZ4yfmCv"
      },
      "source": [
        "# Part 1: Training a model on Gnutella 08"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKHtNFA5EadN"
      },
      "source": [
        "## Preprocessing Gnutella08 dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uKKzWmx3EYbG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-04 19:47:23.887086: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2jSO_ZIiwUFX"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "\n",
        "# choose an embedding size for Structure2Vec\n",
        "EMBED_SIZE = 256\n",
        "\n",
        "# choose number of dense layers in the neural network\n",
        "NUM_LAYERS = 2\n",
        "\n",
        "# choose number of folds for cross validation\n",
        "NUM_FOLD = 5\n",
        "\n",
        "# choose number of epochs for training\n",
        "NUM_EPOCHS = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIuSDB1JEmpg"
      },
      "outputs": [],
      "source": [
        "# Normalize a list of values\n",
        "# NO NEED TO CHANGE\n",
        "\n",
        "def _normalize_array_by_rank(true_value, nr_nodes):\n",
        "  # true_value is a list of values you want to normalize and nr_nodes is the number of nodes in the list\n",
        "\n",
        "  rank = np.argsort(true_value, kind='mergesort', axis=None) #deg list get's normalised\n",
        "  norm = np.empty([nr_nodes])\n",
        "\n",
        "  for i in range(0, nr_nodes):\n",
        "    norm[rank[i]] = float(i+1) / float(nr_nodes)\n",
        "\n",
        "  max = np.amax(norm)\n",
        "  min = np.amin(norm)\n",
        "  if max > 0.0 and max > min:\n",
        "    for i in range(0, nr_nodes):\n",
        "      norm[i] = 2.0*(float(norm[i] - min) / float(max - min)) - 1.0\n",
        "  else:\n",
        "    print(\"Max value = 0\")\n",
        "\n",
        "  return norm, rank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JHQb00GNE0Av"
      },
      "outputs": [],
      "source": [
        "#Read in and create NetworkX Graph; G\n",
        "\n",
        "#TO-DO: The path needs to be changed according to your dataset directory in your GOOGLE DRIVE\n",
        "path = './data/p2p-Gnutella08.txt'\n",
        "\n",
        "G = nx.read_edgelist(path, comments='#', delimiter=None, create_using=nx.DiGraph,\n",
        "                  nodetype=None, data=True, edgetype=None, encoding='utf-8')\n",
        "\n",
        "#print(nx.info(G))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMXmUw9HFL3h",
        "outputId": "e39f7c13-e13e-47c3-d1ed-3178bf8048e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "deg_lst: \n",
            " [10, 1, 6, 87, 69, 83, 1, 60, 84, 59, 7, 15, 4, 7, 17, 12, 2, 15, 1, 3, 2, 79, 2, 11, 11, 2, 13, 7, 6, 1, 11, 54, 95, 50, 80, 70, 90, 91, 80, 92, 19, 90, 75, 79, 23, 52, 30, 5, 5, 7, 17, 23, 12, 16, 21, 14, 1, 2, 9, 87, 76, 21, 67, 48, 91, 56, 14, 1, 6, 6, 16, 2, 39, 4, 1, 17, 20, 2, 4, 97, 71, 81, 94, 91, 40, 8, 10, 14, 6, 6, 5, 12, 4, 2, 67, 57, 69, 11, 1, 4, 12, 15, 5, 6, 3, 4, 13, 2, 1, 13, 2, 10, 15, 2, 3, 1, 12, 5, 9, 14, 37, 15, 25, 4, 5, 13, 3, 12, 2, 6, 5, 2, 7, 13, 5, 4, 19, 7, 16, 83, 72, 19, 14, 8, 6, 16, 20, 5, 3, 2, 2, 14, 20, 12, 12, 11, 11, 11, 12, 2, 2, 13, 1, 30, 12, 1, 5, 2, 16, 11, 9, 5, 15, 3, 1, 3, 4, 7, 4, 15, 17, 7, 2, 18, 1, 8, 15, 4, 6, 13, 3, 10, 2, 12, 3, 7, 10, 4, 1, 3, 3, 17, 7, 5, 1, 5, 4, 14, 7, 6, 3, 2, 4, 4, 11, 2, 12, 10, 11, 5, 4, 12, 11, 12, 20, 15, 14, 2, 1, 5, 14, 12, 14, 13, 3, 14, 12, 12, 3, 27, 2, 11, 2, 2, 14, 9, 6, 3, 13, 1, 1, 5, 1, 1, 4, 7, 1, 2, 2, 9, 12, 4, 4, 5, 17, 9, 14, 25, 13, 4, 2, 5, 7, 11, 4, 5, 4, 4, 1, 10, 12, 12, 5, 1, 6, 1, 15, 1, 14, 17, 13, 9, 9, 9, 5, 2, 4, 10, 14, 18, 16, 14, 1, 10, 20, 4, 13, 15, 1, 12, 13, 11, 18, 4, 3, 9, 13, 3, 16, 1, 1, 1, 2, 15, 16, 11, 13, 2, 3, 14, 4, 16, 1, 3, 13, 8, 5, 2, 4, 11, 11, 11, 7, 4, 2, 12, 4, 10, 3, 1, 77, 25, 13, 1, 45, 11, 1, 14, 1, 3, 4, 4, 12, 3, 15, 4, 18, 6, 4, 4, 1, 5, 15, 6, 9, 3, 10, 2, 4, 7, 3, 13, 6, 19, 15, 72, 82, 76, 44, 5, 3, 1, 1, 13, 17, 12, 11, 1, 14, 2, 3, 16, 15, 4, 5, 4, 13, 15, 15, 3, 2, 4, 11, 10, 18, 14, 16, 21, 4, 7, 15, 9, 4, 4, 8, 14, 1, 2, 2, 15, 2, 3, 14, 6, 11, 15, 19, 27, 7, 3, 14, 6, 13, 18, 5, 2, 1, 13, 12, 13, 14, 14, 89, 9, 14, 6, 11, 14, 4, 15, 12, 12, 79, 82, 3, 4, 2, 9, 2, 4, 5, 6, 3, 3, 69, 79, 65, 3, 21, 10, 20, 13, 11, 3, 1, 1, 13, 3, 10, 4, 6, 11, 66, 22, 2, 24, 4, 16, 4, 13, 12, 3, 2, 2, 13, 15, 4, 12, 13, 3, 12, 2, 3, 16, 5, 11, 16, 13, 3, 12, 3, 20, 16, 11, 17, 12, 3, 20, 13, 12, 14, 4, 3, 3, 1, 1, 12, 2, 6, 4, 7, 4, 10, 13, 3, 7, 2, 5, 2, 5, 12, 8, 8, 14, 16, 3, 88, 2, 10, 25, 65, 16, 13, 3, 12, 13, 3, 12, 1, 72, 51, 13, 2, 1, 12, 7, 4, 21, 4, 11, 2, 14, 15, 12, 1, 2, 2, 1, 2, 1, 1, 3, 14, 4, 19, 15, 14, 9, 8, 16, 1, 14, 2, 16, 2, 3, 13, 19, 13, 14, 4, 1, 2, 91, 5, 4, 4, 10, 11, 2, 1, 9, 2, 1, 13, 17, 43, 11, 13, 10, 14, 22, 13, 2, 2, 2, 2, 41, 15, 1, 15, 7, 32, 3, 6, 3, 12, 1, 3, 3, 3, 9, 16, 3, 2, 1, 11, 12, 12, 5, 11, 5, 1, 1, 4, 2, 1, 15, 3, 1, 1, 2, 1, 2, 1, 1, 5, 10, 11, 11, 14, 11, 1, 1, 15, 4, 1, 3, 1, 3, 2, 2, 2, 41, 21, 16, 1, 12, 1, 11, 12, 11, 14, 9, 6, 12, 10, 7, 3, 2, 3, 1, 21, 10, 18, 2, 16, 6, 2, 5, 6, 3, 12, 3, 3, 12, 3, 12, 2, 2, 1, 2, 13, 11, 1, 3, 3, 3, 8, 2, 1, 5, 5, 8, 2, 2, 17, 8, 4, 20, 2, 5, 5, 15, 3, 4, 11, 2, 4, 3, 6, 11, 12, 8, 5, 12, 22, 6, 5, 10, 6, 1, 14, 2, 15, 2, 11, 3, 2, 3, 14, 14, 17, 5, 5, 2, 1, 2, 5, 9, 7, 13, 8, 3, 6, 15, 13, 10, 1, 1, 4, 10, 1, 14, 1, 13, 1, 1, 8, 1, 5, 2, 3, 3, 3, 1, 16, 11, 5, 9, 12, 4, 5, 14, 7, 4, 3, 3, 12, 2, 5, 3, 2, 1, 1, 16, 1, 3, 1, 17, 13, 1, 8, 14, 11, 3, 12, 14, 12, 1, 8, 26, 14, 1, 8, 1, 3, 1, 5, 2, 3, 2, 3, 9, 6, 3, 1, 19, 1, 8, 10, 19, 6, 6, 12, 1, 3, 1, 15, 4, 15, 13, 6, 2, 11, 12, 15, 4, 13, 3, 4, 4, 13, 3, 1, 7, 12, 4, 4, 1, 7, 4, 13, 1, 1, 3, 7, 8, 2, 16, 5, 16, 15, 9, 8, 12, 11, 2, 1, 13, 4, 9, 2, 14, 3, 13, 15, 4, 3, 5, 5, 7, 3, 7, 14, 5, 7, 4, 2, 1, 2, 13, 1, 16, 8, 25, 4, 14, 11, 14, 2, 3, 19, 8, 2, 5, 2, 3, 5, 9, 4, 16, 11, 80, 32, 13, 5, 4, 20, 12, 1, 2, 12, 11, 4, 5, 17, 1, 12, 1, 16, 7, 9, 4, 16, 11, 15, 11, 2, 4, 12, 5, 5, 9, 2, 15, 1, 4, 1, 3, 7, 7, 5, 12, 13, 7, 1, 3, 5, 4, 10, 3, 2, 4, 2, 1, 1, 1, 9, 12, 12, 5, 2, 22, 16, 1, 3, 6, 6, 5, 2, 17, 1, 3, 8, 12, 1, 15, 14, 1, 12, 1, 1, 2, 3, 3, 7, 2, 4, 16, 7, 12, 1, 1, 12, 14, 4, 6, 13, 6, 3, 12, 14, 20, 12, 6, 5, 4, 8, 11, 2, 11, 12, 2, 11, 7, 14, 5, 14, 7, 13, 10, 16, 12, 6, 17, 16, 2, 11, 6, 16, 1, 3, 2, 11, 4, 3, 3, 16, 13, 4, 12, 17, 13, 2, 12, 14, 16, 3, 3, 1, 6, 13, 13, 7, 14, 1, 11, 7, 3, 3, 13, 16, 2, 1, 12, 14, 3, 16, 3, 4, 14, 14, 14, 15, 2, 18, 2, 17, 1, 16, 13, 5, 4, 7, 4, 13, 12, 4, 2, 13, 21, 8, 2, 1, 10, 12, 11, 2, 1, 1, 11, 3, 14, 11, 1, 4, 11, 2, 29, 5, 15, 11, 2, 12, 12, 17, 1, 16, 13, 2, 1, 17, 13, 14, 3, 14, 8, 16, 1, 15, 6, 3, 14, 10, 11, 8, 1, 1, 4, 1, 13, 7, 6, 5, 19, 10, 14, 1, 7, 13, 13, 7, 13, 2, 13, 20, 4, 2, 18, 7, 18, 2, 17, 7, 18, 12, 2, 1, 1, 4, 3, 12, 10, 9, 13, 1, 9, 15, 11, 2, 2, 16, 12, 14, 13, 23, 4, 2, 3, 10, 4, 12, 2, 1, 3, 2, 6, 7, 1, 13, 17, 14, 10, 5, 12, 16, 1, 13, 2, 3, 9, 4, 16, 5, 4, 1, 12, 1, 18, 16, 4, 5, 6, 4, 2, 3, 1, 1, 3, 3, 10, 15, 2, 13, 12, 1, 1, 1, 1, 1, 5, 22, 1, 2, 1, 2, 11, 10, 8, 3, 11, 7, 8, 17, 3, 1, 1, 6, 1, 2, 1, 13, 3, 9, 3, 3, 16, 4, 4, 13, 1, 12, 18, 10, 3, 12, 11, 8, 13, 7, 3, 8, 13, 16, 3, 2, 3, 3, 15, 2, 7, 6, 4, 11, 16, 1, 4, 12, 12, 5, 6, 4, 15, 19, 4, 15, 8, 13, 4, 1, 10, 17, 1, 9, 1, 1, 1, 14, 11, 5, 20, 13, 4, 9, 12, 1, 14, 1, 6, 11, 8, 7, 7, 2, 10, 1, 1, 2, 18, 12, 3, 1, 7, 5, 4, 2, 8, 2, 4, 2, 6, 5, 3, 16, 16, 1, 5, 12, 9, 10, 15, 12, 18, 1, 7, 8, 15, 15, 12, 8, 13, 5, 1, 1, 5, 1, 1, 1, 13, 14, 3, 19, 12, 12, 12, 1, 5, 13, 14, 3, 1, 13, 14, 2, 1, 4, 1, 1, 11, 11, 13, 14, 3, 15, 18, 2, 13, 6, 13, 12, 5, 6, 10, 2, 1, 6, 12, 16, 4, 5, 5, 15, 1, 6, 11, 16, 1, 24, 24, 2, 6, 5, 5, 1, 5, 3, 4, 1, 15, 11, 6, 1, 2, 12, 15, 2, 1, 11, 12, 1, 2, 6, 1, 12, 1, 4, 14, 10, 1, 7, 1, 14, 9, 3, 5, 1, 12, 13, 11, 7, 18, 9, 3, 22, 5, 9, 3, 3, 5, 6, 6, 5, 6, 2, 4, 13, 9, 1, 11, 3, 3, 13, 6, 11, 3, 16, 1, 5, 4, 7, 13, 6, 11, 4, 12, 12, 2, 10, 7, 1, 3, 11, 2, 1, 12, 1, 1, 16, 1, 6, 12, 2, 1, 1, 11, 2, 1, 1, 6, 3, 1, 2, 15, 4, 5, 16, 16, 1, 1, 4, 13, 10, 5, 5, 6, 1, 15, 5, 5, 14, 2, 1, 12, 12, 1, 1, 18, 14, 2, 13, 22, 13, 3, 4, 11, 2, 1, 5, 3, 1, 4, 3, 7, 1, 4, 25, 1, 1, 14, 14, 22, 3, 14, 1, 13, 1, 14, 5, 14, 13, 2, 12, 2, 11, 1, 3, 12, 2, 1, 2, 7, 4, 2, 12, 12, 3, 2, 15, 13, 4, 12, 3, 3, 4, 1, 8, 14, 14, 3, 4, 5, 5, 11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 11, 1, 4, 2, 18, 13, 13, 5, 9, 4, 4, 3, 11, 2, 12, 12, 11, 1, 2, 3, 4, 7, 1, 4, 12, 20, 2, 4, 11, 2, 6, 15, 9, 14, 11, 3, 5, 11, 1, 3, 12, 1, 3, 1, 1, 4, 10, 3, 13, 13, 3, 13, 12, 16, 14, 1, 6, 12, 12, 7, 4, 4, 2, 1, 11, 1, 13, 1, 14, 6, 7, 14, 9, 2, 1, 1, 3, 4, 14, 14, 2, 3, 2, 1, 8, 14, 2, 11, 10, 5, 6, 2, 7, 15, 1, 13, 12, 18, 10, 2, 1, 2, 14, 6, 12, 13, 4, 15, 4, 5, 3, 12, 1, 5, 1, 11, 2, 4, 4, 1, 1, 7, 20, 11, 6, 12, 2, 11, 9, 1, 13, 3, 1, 2, 2, 2, 3, 2, 4, 3, 12, 12, 11, 2, 13, 7, 15, 1, 1, 1, 11, 11, 13, 10, 5, 6, 4, 1, 2, 1, 2, 14, 3, 2, 3, 2, 5, 10, 3, 8, 6, 2, 3, 3, 1, 2, 1, 12, 11, 1, 1, 1, 12, 15, 2, 16, 6, 8, 12, 14, 12, 12, 1, 12, 3, 2, 16, 8, 2, 23, 3, 13, 25, 14, 2, 11, 1, 2, 1, 11, 2, 12, 13, 12, 17, 2, 4, 15, 15, 4, 3, 12, 13, 2, 16, 13, 1, 2, 3, 15, 10, 8, 5, 12, 5, 13, 3, 4, 1, 10, 3, 5, 4, 3, 4, 5, 2, 11, 1, 4, 7, 4, 21, 1, 6, 3, 12, 12, 11, 3, 2, 9, 12, 1, 11, 4, 3, 14, 21, 13, 14, 13, 13, 13, 13, 5, 11, 3, 1, 11, 1, 4, 17, 2, 11, 1, 3, 15, 2, 2, 14, 6, 7, 3, 7, 2, 6, 3, 1, 2, 9, 7, 7, 12, 11, 1, 5, 1, 9, 3, 4, 5, 17, 13, 4, 4, 1, 15, 18, 14, 16, 5, 14, 3, 16, 12, 12, 10, 12, 2, 14, 2, 2, 3, 17, 4, 3, 3, 13, 1, 1, 3, 4, 10, 4, 3, 14, 1, 19, 2, 5, 14, 10, 1, 12, 3, 1, 5, 11, 12, 1, 4, 14, 1, 2, 7, 6, 13, 13, 7, 4, 18, 2, 1, 16, 4, 12, 1, 3, 7, 2, 1, 11, 1, 3, 4, 19, 13, 4, 3, 12, 1, 14, 2, 3, 2, 1, 2, 2, 1, 11, 13, 2, 12, 12, 11, 5, 2, 2, 2, 1, 4, 10, 3, 13, 9, 20, 24, 3, 1, 5, 1, 12, 11, 2, 1, 9, 13, 2, 7, 14, 1, 2, 2, 4, 2, 1, 11, 15, 17, 13, 9, 2, 12, 2, 12, 14, 6, 4, 2, 13, 2, 2, 13, 13, 1, 5, 5, 12, 1, 3, 13, 3, 3, 5, 14, 10, 12, 4, 2, 2, 13, 2, 5, 5, 2, 3, 2, 13, 12, 1, 14, 14, 5, 2, 15, 2, 16, 11, 3, 14, 13, 7, 1, 13, 1, 16, 13, 12, 7, 3, 3, 1, 1, 12, 16, 7, 2, 5, 3, 3, 5, 5, 1, 4, 2, 4, 2, 15, 4, 3, 13, 1, 5, 1, 1, 1, 4, 2, 6, 8, 2, 1, 5, 4, 2, 5, 6, 1, 1, 5, 3, 1, 2, 5, 2, 1, 2, 2, 14, 11, 1, 10, 1, 2, 2, 6, 1, 7, 7, 4, 1, 1, 2, 15, 10, 2, 1, 13, 1, 14, 1, 6, 5, 15, 3, 5, 2, 2, 5, 4, 3, 13, 4, 13, 14, 16, 6, 1, 4, 3, 14, 13, 5, 3, 11, 12, 2, 13, 3, 14, 14, 15, 3, 1, 1, 3, 12, 6, 3, 13, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 11, 4, 16, 1, 13, 13, 16, 9, 2, 3, 4, 1, 2, 1, 17, 1, 4, 2, 15, 1, 2, 3, 1, 1, 2, 5, 11, 11, 3, 12, 1, 11, 1, 3, 1, 11, 3, 3, 1, 4, 13, 1, 12, 11, 2, 1, 2, 14, 2, 3, 2, 5, 3, 2, 2, 4, 16, 4, 2, 12, 12, 1, 10, 16, 11, 1, 15, 2, 1, 2, 11, 1, 1, 11, 8, 13, 1, 30, 4, 3, 4, 1, 17, 4, 2, 3, 6, 14, 2, 13, 3, 16, 2, 1, 1, 4, 3, 15, 14, 12, 5, 5, 1, 6, 2, 1, 17, 1, 3, 11, 13, 1, 13, 3, 16, 8, 13, 4, 1, 2, 11, 4, 16, 1, 13, 11, 1, 20, 11, 4, 16, 16, 2, 1, 1, 11, 1, 13, 1, 13, 30, 13, 4, 13, 13, 4, 11, 5, 2, 1, 3, 12, 10, 2, 1, 4, 13, 13, 4, 1, 4, 12, 17, 1, 13, 14, 10, 2, 20, 13, 2, 4, 1, 6, 5, 4, 4, 4, 1, 9, 11, 2, 12, 13, 3, 16, 2, 3, 1, 7, 11, 14, 2, 1, 10, 1, 11, 4, 6, 12, 1, 11, 13, 4, 1, 2, 1, 2, 1, 4, 1, 8, 1, 4, 1, 15, 2, 14, 4, 1, 3, 15, 17, 14, 3, 12, 13, 12, 1, 1, 1, 11, 11, 5, 6, 14, 12, 1, 7, 2, 13, 5, 7, 1, 11, 1, 18, 13, 3, 11, 10, 1, 7, 4, 3, 15, 12, 15, 4, 1, 1, 11, 11, 10, 1, 7, 4, 7, 1, 4, 5, 8, 1, 10, 4, 4, 5, 2, 1, 1, 2, 3, 11, 3, 12, 4, 2, 11, 1, 4, 1, 11, 3, 1, 12, 12, 2, 2, 4, 1, 1, 1, 7, 15, 4, 9, 2, 2, 2, 2, 12, 12, 1, 5, 2, 1, 2, 6, 5, 1, 7, 13, 10, 3, 11, 1, 12, 14, 6, 6, 5, 1, 11, 2, 14, 9, 1, 1, 2, 7, 2, 2, 2, 9, 2, 1, 11, 1, 10, 8, 2, 4, 4, 2, 3, 4, 5, 10, 12, 1, 7, 1, 8, 4, 13, 1, 2, 2, 3, 1, 3, 4, 1, 3, 2, 3, 2, 1, 2, 14, 11, 12, 14, 12, 14, 13, 7, 1, 5, 4, 1, 2, 13, 12, 11, 12, 11, 1, 1, 15, 15, 11, 6, 11, 15, 2, 43, 12, 15, 1, 11, 11, 14, 1, 11, 6, 1, 11, 15, 8, 10, 11, 11, 11, 10, 13, 1, 1, 15, 1, 1, 1, 1, 1, 2, 3, 13, 2, 16, 7, 17, 2, 1, 11, 15, 2, 2, 1, 10, 15, 10, 4, 6, 3, 3, 3, 19, 12, 1, 3, 2, 6, 10, 1, 1, 4, 1, 1, 1, 1, 2, 1, 2, 12, 10, 2, 13, 11, 13, 3, 1, 1, 1, 33, 12, 11, 11, 3, 6, 3, 10, 1, 8, 11, 11, 1, 11, 10, 12, 1, 12, 1, 13, 1, 4, 5, 13, 1, 3, 5, 4, 1, 1, 6, 1, 4, 14, 11, 13, 1, 5, 2, 2, 5, 14, 15, 3, 15, 15, 3, 5, 9, 2, 10, 1, 1, 13, 16, 2, 24, 2, 1, 2, 13, 1, 1, 4, 1, 12, 12, 13, 12, 5, 17, 2, 1, 2, 12, 5, 2, 21, 16, 4, 9, 1, 6, 2, 1, 12, 1, 3, 2, 2, 10, 4, 15, 3, 13, 4, 5, 6, 2, 5, 13, 1, 13, 4, 2, 9, 4, 2, 1, 3, 3, 1, 2, 1, 2, 17, 13, 2, 2, 2, 5, 4, 3, 10, 11, 1, 13, 13, 4, 12, 1, 1, 1, 2, 4, 1, 1, 1, 4, 3, 4, 2, 9, 1, 1, 3, 4, 5, 11, 8, 10, 14, 12, 1, 12, 18, 3, 15, 13, 1, 4, 1, 16, 10, 2, 1, 21, 1, 16, 2, 14, 3, 11, 2, 1, 1, 11, 10, 6, 14, 1, 1, 1, 3, 1, 1, 23, 11, 2, 3, 4, 1, 1, 2, 11, 2, 3, 1, 13, 1, 12, 6, 5, 4, 3, 3, 8, 13, 2, 11, 3, 1, 1, 5, 12, 14, 4, 7, 2, 4, 12, 1, 2, 7, 1, 2, 6, 3, 14, 12, 1, 2, 12, 1, 10, 1, 14, 6, 6, 14, 11, 8, 2, 2, 1, 1, 3, 16, 1, 1, 12, 2, 11, 4, 4, 16, 2, 3, 4, 4, 13, 6, 18, 7, 11, 1, 4, 16, 4, 12, 1, 11, 2, 4, 1, 11, 5, 13, 12, 10, 13, 4, 13, 13, 13, 4, 4, 4, 2, 10, 1, 1, 14, 4, 13, 3, 1, 6, 3, 2, 3, 3, 3, 12, 1, 11, 1, 15, 4, 2, 16, 14, 2, 12, 11, 5, 1, 7, 11, 14, 13, 2, 4, 13, 2, 11, 15, 2, 1, 2, 1, 13, 1, 1, 2, 10, 11, 13, 5, 12, 13, 12, 1, 10, 12, 2, 1, 3, 1, 2, 6, 11, 3, 1, 2, 12, 14, 1, 13, 7, 3, 2, 12, 2, 2, 2, 3, 8, 4, 11, 12, 1, 1, 5, 11, 3, 12, 12, 13, 2, 3, 12, 1, 12, 1, 7, 6, 1, 4, 1, 3, 16, 13, 2, 5, 3, 6, 3, 2, 1, 1, 10, 1, 8, 1, 1, 1, 1, 10, 1, 10, 19, 2, 1, 13, 6, 2, 1, 10, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 11, 2, 11, 1, 2, 3, 5, 1, 6, 2, 10, 1, 4, 1, 11, 1, 3, 3, 10, 2, 1, 10, 1, 3, 1, 3, 12, 16, 12, 1, 1, 2, 1, 2, 2, 11, 12, 2, 15, 10, 1, 1, 1, 3, 4, 5, 10, 5, 14, 1, 3, 11, 1, 13, 3, 6, 10, 12, 17, 3, 14, 4, 12, 2, 2, 11, 12, 7, 5, 3, 1, 8, 3, 1, 9, 3, 1, 3, 14, 10, 12, 1, 2, 2, 2, 2, 16, 1, 11, 3, 13, 2, 1, 8, 14, 12, 11, 12, 2, 7, 1, 13, 1, 10, 3, 11, 1, 3, 3, 1, 2, 12, 11, 11, 3, 1, 12, 4, 14, 3, 12, 3, 1, 9, 1, 10, 3, 12, 7, 2, 2, 11, 11, 2, 16, 7, 1, 12, 3, 1, 3, 1, 12, 2, 2, 10, 3, 4, 12, 14, 11, 11, 4, 2, 5, 1, 3, 4, 2, 3, 12, 2, 3, 1, 1, 1, 14, 10, 10, 10, 12, 7, 2, 1, 10, 1, 2, 6, 2, 10, 1, 3, 15, 2, 1, 1, 3, 2, 11, 1, 3, 9, 3, 1, 3, 1, 1, 15, 10, 3, 1, 9, 11, 4, 11, 13, 4, 13, 1, 1, 1, 4, 13, 4, 13, 6, 14, 3, 10, 12, 1, 1, 7, 2, 15, 2, 12, 12, 13, 11, 12, 2, 14, 11, 5, 2, 3, 11, 3, 13, 5, 3, 11, 15, 1, 10, 50, 12, 4, 1, 2, 1, 2, 1, 3, 2, 3, 1, 1, 1, 1, 12, 7, 2, 5, 1, 13, 1, 1, 1, 1, 2, 1, 1, 13, 1, 11, 4, 4, 3, 17, 2, 1, 10, 1, 11, 3, 1, 2, 13, 1, 1, 1, 6, 1, 1, 11, 12, 3, 1, 14, 1, 12, 13, 3, 3, 3, 4, 1, 9, 2, 14, 12, 11, 9, 2, 4, 14, 13, 4, 1, 1, 2, 1, 11, 1, 3, 1, 12, 13, 2, 3, 13, 12, 3, 2, 1, 10, 7, 10, 13, 2, 11, 6, 11, 1, 1, 1, 12, 11, 7, 17, 1, 1, 10, 13, 12, 11, 10, 4, 2, 4, 10, 12, 3, 1, 11, 11, 1, 13, 2, 11, 9, 7, 1, 11, 1, 5, 1, 4, 1, 1, 11, 12, 8, 2, 1, 8, 10, 2, 3, 12, 1, 3, 1, 11, 10, 4, 16, 1, 13, 1, 3, 6, 3, 1, 6, 1, 14, 2, 1, 3, 1, 1, 2, 13, 1, 10, 3, 9, 8, 12, 1, 3, 9, 12, 2, 14, 1, 1, 12, 1, 2, 11, 4, 3, 3, 11, 2, 2, 1, 12, 11, 11, 5, 12, 1, 1, 4, 1, 12, 14, 12, 12, 12, 11, 11, 13, 12, 2, 12, 1, 11, 3, 1, 13, 12, 1, 11, 1, 3, 12, 1, 1, 2, 1, 2, 11, 12, 2, 1, 9, 1, 3, 11, 3, 3, 4, 1, 1, 12, 1, 11, 3, 3, 1, 11, 5, 2, 3, 1, 4, 14, 2, 2, 10, 11, 1, 1, 13, 16, 6, 3, 1, 3, 9, 2, 1, 1, 2, 4, 9, 11, 1, 16, 17, 2, 1, 1, 4, 10, 3, 3, 2, 7, 4, 1, 1, 15, 2, 1, 6, 12, 4, 10, 1, 5, 11, 11, 12, 3, 2, 15, 1, 1, 2, 3, 1, 1, 1, 10, 2, 1, 12, 12, 12, 5, 2, 2, 3, 1, 13, 15, 1, 2, 7, 6, 2, 11, 4, 13, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 13, 1, 12, 2, 1, 1, 1, 1, 1, 1, 1, 3, 5, 2, 3, 4, 3, 3, 1, 1, 4, 1, 1, 11, 1, 1, 11, 3, 5, 12, 1, 2, 5, 17, 10, 1, 11, 2, 2, 3, 5, 3, 5, 1, 2, 1, 12, 2, 3, 14, 5, 3, 1, 2, 1, 3, 8, 1, 2, 2, 12, 15, 11, 3, 1, 1, 2, 4, 10, 4, 11, 1, 5, 2, 2, 2, 1, 2, 1, 11, 4, 2, 1, 1, 10, 2, 2, 3, 2, 1, 1, 2, 2, 1, 2, 1, 4, 11, 2, 3, 8, 2, 2, 1, 1, 17, 7, 2, 1, 12, 7, 1, 1, 3, 11, 1, 1, 3, 3, 11, 12, 1, 1, 3, 6, 1, 11, 1, 15, 13, 5, 3, 11, 14, 1, 1, 1, 1, 13, 2, 13, 14, 12, 2, 1, 19, 1, 13, 1, 18, 14, 2, 1, 2, 1, 1, 3, 1, 11, 1, 13, 2, 14, 9, 1, 1, 1, 2, 3, 11, 3, 4, 1, 2, 2, 1, 12, 11, 1, 1, 3, 7, 14, 1, 13, 3, 15, 1, 12, 14, 12, 6, 4, 2, 1, 1, 2, 1, 1, 4, 14, 11, 1, 1, 3, 1, 1, 11, 2, 12, 1, 6, 1, 1, 12, 13, 1, 1, 15, 13, 3, 1, 5, 3, 3, 1, 11, 1, 13, 12, 13, 3, 2, 1, 3, 2, 7, 1, 1, 4, 2, 1, 1, 3, 13, 2, 1, 1, 4, 3, 3, 13, 11, 14, 6, 2, 5, 3, 12, 1, 2, 12, 14, 8, 2, 13, 13, 3, 1, 13, 1, 1, 1, 1, 2, 12, 1, 12, 11, 2, 10, 1, 3, 1, 4, 4, 1, 4, 1, 4, 7, 11, 12, 11, 15, 12, 4, 1, 1, 2, 4, 2, 1, 1, 1, 3, 3, 1, 4, 1, 1, 2, 1, 12, 12, 14, 15, 1, 4, 10, 2, 1, 1, 1, 2, 2, 10, 1, 13, 2, 1, 11, 11, 1, 13, 5, 4, 3, 10, 2, 12, 14, 13, 4, 3, 6, 2, 11, 1, 2, 6, 2, 15, 12, 11, 5, 11, 1, 15, 2, 3, 1, 12, 4, 3, 2, 2, 11, 3, 1, 2, 1, 5, 2, 1, 2, 1, 2, 6, 12, 12, 1, 12, 4, 12, 2, 12, 2, 1, 3, 3, 2, 13, 4, 3, 2, 7, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 13, 3, 10, 10, 3, 12, 3, 12, 2, 3, 1, 2, 14, 1, 11, 2, 11, 4, 5, 12, 3, 12, 1, 12, 6, 3, 2, 2, 1, 1, 1, 1, 2, 15, 2, 7, 1, 1, 2, 13, 5, 3, 2, 12, 1, 4, 14, 2, 11, 12, 6, 13, 1, 1, 15, 11, 13, 12, 2, 2, 3, 3, 1, 3, 3, 1, 14, 2, 2, 1, 5, 5, 8, 11, 2, 10, 1, 2, 2, 3, 4, 4, 4, 4, 2, 1, 14, 3, 1, 1, 12, 11, 15, 16, 12, 1, 12, 6, 3, 2, 4, 2, 14, 1, 14, 3, 1, 1, 5, 11, 1, 2, 2, 1, 13, 1, 3, 11, 2, 12, 2, 1, 11, 11, 2, 1, 1, 2, 9, 11, 2, 1, 12, 1, 15, 1, 12, 2, 13, 7, 1, 12, 12, 10, 3, 10, 3, 14, 11, 2, 3, 11, 1, 14, 2, 12, 12, 11, 1, 2, 11, 2, 2, 1, 12, 11, 1, 1, 3, 12, 2, 6, 1, 10, 12, 1, 6, 5, 4, 2, 11, 11, 1, 11, 1, 3, 13, 12, 11, 2, 2, 2, 2, 12, 1, 11, 1, 4, 10, 2, 11, 13, 3, 1, 1, 13, 1, 3, 2, 4, 1, 2, 1, 2, 2, 3, 4, 2, 1, 3, 11, 12, 11, 1, 2, 11, 14, 1, 1, 20, 4, 16, 1, 15, 3, 12, 3, 14, 10, 2, 12, 1, 11, 1, 15, 3, 1, 13, 34, 11, 12, 5, 11, 1, 1, 2, 11, 1, 1, 3, 4, 1, 2, 2, 1, 15, 4, 9, 1, 1, 1, 13, 1, 2, 3, 1, 13, 1, 14, 6, 1, 13, 2, 1, 1, 2, 1, 2, 1, 2, 12, 2, 13, 7, 8, 12, 4, 11, 6, 1, 11, 2, 14, 3, 3, 1, 10, 1, 10, 4, 2, 1, 12, 1, 7, 2, 1, 4, 1, 4, 1, 1, 1, 3, 1, 2, 12, 1, 3, 10, 16, 2, 1, 1, 1, 1, 13, 11, 13, 1, 12, 10, 1, 7, 11, 2, 1, 11, 1, 1, 3, 1, 5, 2, 2, 1, 14, 11, 2, 11, 11, 1, 1, 1, 2, 7, 13, 11, 3, 2, 12, 11, 1, 3, 14, 6, 13, 11, 2, 2, 2, 12, 2, 4, 1, 12, 11, 11, 13, 3, 1, 11, 2, 2, 11, 4, 1, 10, 12, 11, 3, 1, 11, 13, 1, 3, 3, 1, 2, 2, 13, 11, 2, 11, 1, 7, 13, 11, 1, 1, 13, 1, 3, 3, 1, 2, 5, 3, 1, 11, 2, 2, 16, 1, 12, 3, 2, 3, 2, 10, 3, 3, 12, 2, 1, 11, 13, 5, 1, 2, 2, 4, 13, 1, 11, 2, 2, 11, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 2, 2, 2, 14, 3, 1, 1, 10, 12, 1, 1, 1, 2, 8, 1, 14, 2, 1, 1, 9, 13, 2, 1, 1, 13, 1, 1, 5, 1, 2, 1, 11, 2, 3, 1, 1, 1, 11, 11, 9, 11, 2, 1, 2, 11, 1, 1, 11, 5, 1, 2, 3, 13, 1, 13, 11, 4, 1, 1, 4, 1, 3, 11, 1, 1, 11, 3, 12, 2, 1, 1, 4, 1, 1, 12, 2, 2, 1, 1, 1, 1, 3, 2, 1, 1, 12, 1, 12, 1, 2, 1, 2, 2, 1, 15, 12, 1, 11, 2, 5, 12, 1, 12, 1, 11, 9, 1, 11, 2, 3, 1, 1, 1, 1, 10, 1, 2, 15, 8, 1, 3, 11, 1, 11, 3, 2, 2, 2, 1, 11, 8, 12, 11, 11, 2, 11, 3, 1, 1, 1, 5, 1, 2, 7, 2, 3, 6, 1, 1, 1, 12, 3, 1, 12, 1, 10, 1, 2, 6, 12, 13, 1, 1, 1, 1, 1, 1, 3, 1, 2, 11, 2, 1, 1, 1, 1, 2, 1, 2, 1, 3, 2, 12, 12, 1, 14, 10, 6, 11, 6, 12, 3, 11, 2, 11, 13, 4, 1, 1, 11, 13, 1, 1, 12, 13, 12, 1, 13, 1, 16, 1, 12, 12, 1, 11, 1, 11, 3, 3, 10, 1, 4, 13, 3, 4, 2, 1, 2, 1, 6, 3, 1, 2, 2, 12, 1, 3, 12, 4, 8, 2, 1, 1, 1, 3, 1, 7, 11, 3, 1, 12, 13, 13, 1, 1, 1, 15, 2, 11, 1, 11, 3, 3, 2, 1, 2, 2, 2, 12, 11, 11, 4, 2, 10, 1, 1, 3, 12, 4, 3, 14, 2, 11, 1, 1, 1, 1, 1, 2, 11, 1, 12, 1, 2, 1, 1, 11, 1, 1, 1, 13, 2, 1, 1, 1, 11, 1, 2, 2, 5, 10, 11, 1, 1, 1, 1, 2, 1, 1, 11, 1, 2, 4, 3, 11, 12, 5, 1, 13, 3, 4, 2, 9, 1, 1, 1, 4, 12, 1, 4, 1, 4, 11, 1, 1, 11, 1, 11, 11, 4, 1, 1, 13, 6, 2, 3, 4, 11, 1, 1, 3, 5, 2, 2, 2, 2, 1, 7, 2, 1, 2, 1, 2, 1, 1, 5, 2, 3, 1, 2, 11, 1, 1, 1, 1, 4, 1, 2, 1, 11, 1, 4, 11, 1, 11, 1, 3, 13, 1, 2, 2, 1, 1, 2, 10, 3, 12, 1, 1, 2, 1, 2, 6, 4, 12, 6, 1, 11, 3, 4, 1, 2, 2, 1, 1, 1, 3, 10, 1, 2, 1, 1, 1, 2, 1, 3, 1, 2, 14, 2, 11, 1, 1, 2, 1, 1, 3, 5, 2, 1, 2, 2, 1, 10, 1, 1, 1, 11, 3, 1, 1, 11, 3, 11, 2, 1, 3, 1, 1, 11, 14, 3, 1, 2, 1, 1, 2, 11, 1, 2, 2, 11, 1, 2, 1, 10, 2, 4, 1, 14, 1, 1, 12, 11, 1, 1, 1, 2, 11, 4, 3, 2, 3, 11, 11, 1, 2, 1, 1, 11, 1, 1, 11, 11, 2, 2, 1, 12, 2, 2, 1, 1, 12, 1, 12, 1, 12, 1, 1, 2, 12, 11, 6, 4, 11, 1, 3, 11, 5, 12, 1, 3, 1, 11, 2, 1, 1, 2, 1, 1, 2, 2, 1, 13, 1, 3, 3, 12, 12, 11, 1, 3, 1, 1, 4, 1, 13, 1, 1, 11, 4, 1, 2, 3, 1, 1, 12, 1, 11, 1, 1, 11, 1, 2, 1, 11, 1, 7, 2, 1, 1, 1, 1, 2, 10, 11, 1, 10, 3, 2, 2, 2, 11, 1, 1, 11, 13, 2, 1, 1, 3, 11, 3, 2, 3, 12, 1, 2, 2, 11, 1, 1, 11, 12, 11, 1, 12, 11, 47, 1, 2, 1, 11, 11, 1, 1, 15, 1, 9, 10, 1, 2, 1, 1, 1, 13, 11, 2, 1, 1, 2, 11, 11, 3, 3, 1, 2, 1, 1, 1, 1, 1, 2, 11, 1, 11, 13, 11, 1, 6, 1, 2, 11, 1, 1, 1, 1, 2, 1, 4, 1, 1, 11, 1, 2, 2, 2, 6, 1, 11, 2, 6, 1, 13, 4, 11, 2, 6, 1, 1, 2, 14, 1, 12, 1, 3, 2, 7, 2, 11, 2, 12, 1, 1, 2, 6, 3, 1, 2, 10, 1, 2, 1, 1, 2, 13, 2, 1, 11, 1, 2, 1, 13, 1, 2, 1, 6, 2, 1, 1, 1, 2, 1, 1, 12, 1, 2, 12, 2, 2, 2, 3, 2, 1, 18, 2, 2, 1, 13, 1, 1, 6, 5, 1, 11, 1, 1, 1, 1, 1, 1, 1, 1, 10, 1, 3, 2, 5, 1, 1, 2, 14, 1, 1, 1, 1, 12, 2, 10, 1, 2, 2, 1, 1, 11, 1, 2, 1, 3, 1, 2, 1, 1, 1, 1, 1, 13, 1, 4, 1, 1, 11, 1, 1, 1, 3, 1, 3, 1, 11, 3, 1, 1, 1, 2, 1, 11, 1, 13, 1, 2, 1, 1, 12, 1, 13, 1, 1, 11, 2, 3, 10, 1, 1, 1, 1, 1, 1, 2, 1, 4, 6, 3, 1, 1, 10, 2, 10, 1, 1, 2, 4, 11, 1, 1, 1, 3, 1, 1, 1, 11, 1, 11, 11, 4, 1, 1, 2, 1, 1, 11, 1, 11, 11, 7, 2, 11, 1, 13, 2, 3, 12, 1, 15, 10, 2, 1, 1, 3, 11, 3, 1, 2, 1, 1, 3, 1, 1, 12, 13, 11, 10, 1, 1, 2, 12, 2, 11, 1, 19, 4, 1, 6, 1, 2, 1, 1, 1, 12, 4, 1, 6, 2, 3, 2, 1, 1, 1, 1, 2, 1, 1, 11, 2, 2, 1, 1, 1, 1, 3, 11, 1, 7, 1, 3, 2, 1, 1, 2, 1, 11, 6, 1, 1, 3, 2, 1, 1, 6, 1, 1, 3, 2, 3, 12, 1, 3, 1, 2, 11, 1, 1, 1, 2, 11, 3, 13, 12, 11, 11, 1, 1, 1, 1, 2, 4, 11, 1, 10, 13, 3, 6, 1, 3, 1, 1, 1, 2, 2, 1, 11, 1, 1, 1, 1, 2, 10, 1, 11, 3, 2, 12, 1, 1, 11, 1, 1, 1, 1, 1, 11, 6, 1, 11, 1, 11, 1, 1, 3, 2, 1, 1, 1, 2, 1, 1, 1, 1, 11, 5, 1, 1, 1, 1, 3, 1, 1, 1, 1, 2, 2, 1, 3, 1, 1, 2, 2, 2, 11, 1, 2, 2, 1, 1, 2, 10, 11, 1, 16, 1, 1, 1, 1, 2, 1, 11, 1, 1, 1, 1, 2, 1, 1, 2, 11, 1, 1, 2, 2, 2, 1, 2, 11, 2, 1, 11, 12, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 10, 1, 11, 1, 10, 2, 3, 3, 1, 1, 1, 2, 1, 1, 1, 19, 1, 2, 1, 13, 10, 2, 2, 1, 2, 2, 12, 11, 11, 1, 1, 1, 2, 3, 1, 1, 11, 10, 1, 11, 11, 11, 1, 1, 1, 7, 5, 1, 1, 11, 12, 1, 3, 2, 11, 49, 1, 1, 6, 2, 1, 11, 1, 5, 1, 2, 1, 1, 1, 3, 10, 1, 1, 11, 11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 10, 11, 2, 1, 1, 1, 1, 11, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 14, 2, 1, 11, 11, 1, 11, 2, 1, 11, 1, 11, 1, 10, 1, 1, 2, 1, 5, 1, 11, 1, 1, 2, 11, 11, 1, 11, 1, 2, 1, 10, 1, 1, 1, 3, 1, 1, 1, 11, 2, 1, 11, 1, 1, 1, 1, 1, 11, 11, 12, 1, 11, 11, 5, 12, 1, 2, 1, 1, 11, 1, 1, 11, 2, 1, 1, 1, 1, 2, 12, 12, 12, 2, 12, 11, 11, 1, 1, 1, 1, 11, 14, 2, 11, 1, 2, 1, 11, 2, 1, 1, 1, 2, 1, 1, 10, 2, 10, 2, 2, 6, 12, 1, 1, 11, 2, 1, 11, 11, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1, 2, 12, 2, 1, 11, 11, 1, 1, 1, 1, 2, 1, 1, 1, 11, 1, 2, 11, 10, 1, 10, 11, 1, 1, 1, 1, 1, 11, 11, 10, 1, 9, 1, 1, 1, 1, 2, 10, 2, 1, 2, 10, 10, 3, 11, 12, 1, 2, 12, 1, 1, 11, 1, 1, 1, 11, 10, 2, 1, 1, 1, 1, 11, 1, 1, 1, 2, 2, 2, 11, 11, 11, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 11, 1, 11, 3, 42, 1, 11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 11, 1, 1, 10, 1, 1, 11, 1, 11, 11, 11, 11, 12, 2, 2, 1, 11, 10, 2, 1, 1, 2, 11, 1, 1, 1, 1, 11, 1, 1, 2, 1, 10, 5, 1, 1, 1, 12, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 11, 1, 1, 1, 2, 1, 1, 11, 1, 1, 2, 3, 11, 1, 1, 1, 1, 1, 1, 11, 1, 1, 1, 2, 1, 1, 1, 9, 1, 10, 1, 11, 1, 11, 1, 1, 1, 10, 1, 1, 3, 1, 11, 6, 1, 1, 1, 1, 1, 2, 6, 1, 10, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "# Creating list of Degrees of the nodes in G and normalising them:\n",
        "\n",
        "deg_lst = [val for (node, val) in G.degree()]\n",
        "nr_nodes = G.number_of_nodes()\n",
        "print(\"deg_lst: \\n\", deg_lst)\n",
        "\n",
        "degree_norm, degree_rank = _normalize_array_by_rank(deg_lst, nr_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "B4a79bglFRux"
      },
      "outputs": [],
      "source": [
        "# Computing Ground-truth values and normalising them:\n",
        "\n",
        "b = [v for v in nx.betweenness_centrality(G).values()]\n",
        "\n",
        "BC_norm_cent, BC_cent_rank = _normalize_array_by_rank(b, nr_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0q-1gU8FO9z6"
      },
      "outputs": [],
      "source": [
        "# Define Structure2Vec\n",
        "# NO NEED TO CHANGE\n",
        "\n",
        "def Structure2Vec(G, nr_nodes, degree_norm, num_features=1, embed_size=512, layers=2):\n",
        "\n",
        "  #build feature matrix\n",
        "  def get_degree(i):\n",
        "    return degree_norm[i]\n",
        "\n",
        "  def build_feature_matrix():\n",
        "    n = nr_nodes\n",
        "    feature_matrix = []\n",
        "    for i in range(0, n):\n",
        "      feature_matrix.append(get_degree(i))\n",
        "    return feature_matrix\n",
        "  #Structure2Vec node embedding\n",
        "  A = nx.to_numpy_array(G)\n",
        "\n",
        "  dim = [nr_nodes, num_features]\n",
        "\n",
        "\n",
        "  node_features = tf.cast(build_feature_matrix(), tf.float32)\n",
        "  node_features = tf.reshape(node_features, dim)\n",
        "\n",
        "  initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0,\n",
        "                                                                mode=\"fan_avg\",\n",
        "                                                                distribution=\"uniform\")\n",
        "  #print(initializer)\n",
        "\n",
        "  A = tf.sparse.from_dense(A)\n",
        "  A = tf.cast(A, tf.float32)\n",
        "  w1 = tf.Variable(initializer((num_features, embed_size)), trainable=True,\n",
        "                                  dtype=tf.float32, name=\"w1\")\n",
        "  w2 = tf.Variable(initializer((embed_size, embed_size)), trainable=True,\n",
        "                                  dtype=tf.float32, name=\"w2\")\n",
        "  w3 = tf.Variable(initializer((1,embed_size)), trainable=True, dtype=tf.float32, name=\"w3\")\n",
        "  w4 = tf.Variable(initializer([]), trainable=True, dtype=tf.float32, name=\"w4\")\n",
        "\n",
        "  wx_all = tf.matmul(node_features, w1)  # NxE\n",
        "\n",
        "  #computing X1:\n",
        "  #sparse.reduce_sum: Computes the sum of elements across dimensions of a SparseTensor.\n",
        "  weight_sum_init = tf.sparse.reduce_sum(A, axis=1, keepdims=True, ) #takes adjacency matrix\n",
        "  n_nodes = tf.shape(input=A)[1]\n",
        "\n",
        "  weight_sum = tf.multiply(weight_sum_init, w4)\n",
        "  weight_sum = tf.nn.relu(weight_sum)  # Nx1\n",
        "  weight_sum = tf.matmul(weight_sum, w3)  # NxE\n",
        "\n",
        "  weight_wx = tf.add(wx_all, weight_sum)\n",
        "  current_mu = tf.nn.relu(weight_wx)  # NxE = H^0\n",
        "\n",
        "  for i in range(0, layers):\n",
        "    neighbor_sum = tf.sparse.sparse_dense_matmul(A, current_mu)\n",
        "    neighbor_linear = tf.matmul(neighbor_sum, w2)  # NxE\n",
        "\n",
        "    current_mu = tf.nn.relu(tf.add(neighbor_linear, weight_wx))  # NxE\n",
        "\n",
        "  mu_all = current_mu\n",
        "\n",
        "  return mu_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "47p_VK_dRC5f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1770263295.450779 2673770 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7381 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
            "2026-02-04 19:48:15.614226: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 317620808 exceeds 10% of free system memory.\n"
          ]
        }
      ],
      "source": [
        "# Converting the graph structure into vectors\n",
        "\n",
        "mu_all = Structure2Vec(G, nr_nodes, degree_norm, embed_size=EMBED_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J-8ogrRGFoO"
      },
      "source": [
        "## Training a Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "mwflrFN6F6sx",
        "outputId": "5a76c9d6-31e6-4317-d0a1-83b757740bd6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">49,537</span> (193.50 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m49,537\u001b[0m (193.50 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">49,537</span> (193.50 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m49,537\u001b[0m (193.50 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Building NN model\n",
        "\n",
        "UNITS = int(EMBED_SIZE/2)\n",
        "def build_model():\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.Input(shape=(EMBED_SIZE,)))\n",
        "\n",
        "  # choose the number of layers to construct your network\n",
        "  for _ in range(NUM_LAYERS):\n",
        "    model.add(tf.keras.layers.Dense(UNITS, activation =\"relu\"))\n",
        "\n",
        "  model.add(tf.keras.layers.Dense(1))\n",
        "  model.compile(optimizer='sgd', loss='mse')\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  return model\n",
        "\n",
        "model = build_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSI-9-czGT9j",
        "outputId": "70b712cb-8fa5-4413-c8bb-403c5eb859b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([6301  256], shape=(2,), dtype=int32)\n",
            "tf.Tensor([6301], shape=(1,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "# Construct training set and groundtruth\n",
        "\n",
        "x_train = mu_all\n",
        "y_train = BC_norm_cent\n",
        "print(tf.shape(x_train))\n",
        "print(tf.shape(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ci1l6ODgGVX5",
        "outputId": "7a31cb9c-b9be-46b6-a411-985d2fea0eda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processing fold # 0\n",
            "tf.Tensor([5041  256], shape=(2,), dtype=int32)\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-04 19:48:16.684692: I external/local_xla/xla/service/service.cc:163] XLA service 0x7fa970005040 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2026-02-04 19:48:16.684703: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce GTX 1080, Compute Capability 6.1\n",
            "2026-02-04 19:48:16.703326: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2026-02-04 19:48:16.732864: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91900\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m 185/5041\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 821us/step - loss: 0.2709 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1770263296.888119 2674657 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m5041/5041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 786us/step - loss: 0.0956\n",
            "Epoch 2/5\n",
            "\u001b[1m5041/5041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 792us/step - loss: 0.0704\n",
            "Epoch 3/5\n",
            "\u001b[1m5041/5041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 787us/step - loss: 0.0640\n",
            "Epoch 4/5\n",
            "\u001b[1m5041/5041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 788us/step - loss: 0.0605\n",
            "Epoch 5/5\n",
            "\u001b[1m5041/5041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 791us/step - loss: 0.0580\n",
            "model.metrics_names:  ['loss']\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2033 \n",
            "[0.2033482939004898]\n",
            "processing fold # 1\n",
            "tf.Tensor([5041  256], shape=(2,), dtype=int32)\n",
            "Epoch 1/5\n",
            "\u001b[1m5041/5041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 793us/step - loss: 0.0837\n",
            "Epoch 2/5\n",
            "\u001b[1m5041/5041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 790us/step - loss: 0.0821\n",
            "Epoch 3/5\n",
            "\u001b[1m5041/5041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 793us/step - loss: 0.0795\n",
            "Epoch 4/5\n",
            "\u001b[1m5041/5041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 827us/step - loss: 0.0786\n",
            "Epoch 5/5\n",
            "\u001b[1m5041/5041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 785us/step - loss: 0.0785\n",
            "model.metrics_names:  ['loss']\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0836 \n",
            "[0.2033482939004898, 0.08358737081289291]\n",
            "processing fold # 2\n",
            "tf.Tensor([5041  256], shape=(2,), dtype=int32)\n",
            "Epoch 1/5\n",
            "\u001b[1m5041/5041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 788us/step - loss: 0.0789\n",
            "Epoch 2/5\n",
            "\u001b[1m5041/5041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 794us/step - loss: 0.0779\n",
            "Epoch 3/5\n",
            "\u001b[1m5041/5041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 789us/step - loss: 0.0754\n",
            "Epoch 4/5\n",
            "\u001b[1m5041/5041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 790us/step - loss: 0.0731\n",
            "Epoch 5/5\n",
            "\u001b[1m5041/5041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 798us/step - loss: 0.0728\n",
            "model.metrics_names:  ['loss']\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0462 \n",
            "[0.2033482939004898, 0.08358737081289291, 0.046231284737586975]\n",
            "processing fold # 3\n",
            "tf.Tensor([5041  256], shape=(2,), dtype=int32)\n",
            "Epoch 1/5\n",
            "\u001b[1m5041/5041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 783us/step - loss: 0.0725\n",
            "Epoch 2/5\n",
            "\u001b[1m5041/5041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 783us/step - loss: 0.0720\n",
            "Epoch 3/5\n",
            "\u001b[1m5041/5041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 793us/step - loss: 0.0701\n",
            "Epoch 4/5\n",
            "\u001b[1m5041/5041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 802us/step - loss: 0.0679\n",
            "Epoch 5/5\n",
            "\u001b[1m5041/5041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 784us/step - loss: 0.0673\n",
            "model.metrics_names:  ['loss']\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0479 \n",
            "[0.2033482939004898, 0.08358737081289291, 0.046231284737586975, 0.0478602834045887]\n",
            "processing fold # 4\n",
            "tf.Tensor([5041  256], shape=(2,), dtype=int32)\n",
            "Epoch 1/5\n",
            "\u001b[1m5041/5041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 805us/step - loss: 0.0536\n",
            "Epoch 2/5\n",
            "\u001b[1m5041/5041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 799us/step - loss: 0.0533\n",
            "Epoch 3/5\n",
            "\u001b[1m5041/5041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 790us/step - loss: 0.0521\n",
            "Epoch 4/5\n",
            "\u001b[1m5041/5041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 821us/step - loss: 0.0517\n",
            "Epoch 5/5\n",
            "\u001b[1m5041/5041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 800us/step - loss: 0.0511\n",
            "model.metrics_names:  ['loss']\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1617 \n",
            "[0.2033482939004898, 0.08358737081289291, 0.046231284737586975, 0.0478602834045887, 0.1616974174976349]\n"
          ]
        }
      ],
      "source": [
        "# Computing cross validation\n",
        "# NO NEED TO CHANGE\n",
        "all_scores = []\n",
        "k = NUM_FOLD\n",
        "num_val_samples = len(x_train) // k\n",
        "for i in range(k):\n",
        "  print('processing fold #', i)\n",
        "  val_data = x_train[i*num_val_samples: (i+1) * num_val_samples]\n",
        "  val_targets = y_train[i*num_val_samples: (i+1)*num_val_samples]\n",
        "\n",
        "  partial_train_data = np.concatenate(\n",
        "      [x_train[:i*num_val_samples],\n",
        "      x_train[(i+1)*num_val_samples:]],\n",
        "      axis = 0)\n",
        "  print(tf.shape(partial_train_data))\n",
        "\n",
        "  partial_train_targets = np.concatenate(\n",
        "      [y_train[:i*num_val_samples],\n",
        "      y_train[(i+1)*num_val_samples:]],\n",
        "      axis = 0)\n",
        "\n",
        "  # Training\n",
        "  callbacks =  tf.keras.callbacks.EarlyStopping(\n",
        "      monitor= 'loss', min_delta=0, patience=3, verbose=1,\n",
        "      mode='auto', baseline=None, restore_best_weights=False)\n",
        "\n",
        "  model.fit(partial_train_data, partial_train_targets,\n",
        "            epochs = NUM_EPOCHS, batch_size = 1, callbacks = callbacks, verbose = 1)\n",
        "  print(\"model.metrics_names: \", model.metrics_names)\n",
        "\n",
        "  val_loss = model.evaluate(val_data, val_targets, verbose = 1)\n",
        "\n",
        "  all_scores.append(val_loss)\n",
        "  print(all_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "D6P1Y4USGzLk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n"
          ]
        }
      ],
      "source": [
        "# Computing Kendall on trained set\n",
        "\n",
        "x_new = x_train\n",
        "y_pred = model.predict(x_new)\n",
        "\n",
        "# compute kendalltau using the prediction results and the groundtruth\n",
        "from scipy import stats\n",
        "kendall_tau, p_value = scipy.stats.kendalltau(BC_norm_cent,y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kTI2M_cyJCqB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7299190148642852\n"
          ]
        }
      ],
      "source": [
        "# Print your kendalltau score\n",
        "# Make sure your kendalltau score is at least 0.70\n",
        "# PRINT HERE\n",
        "print(kendall_tau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "brFQI-QBgYc1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "# You could save this model for part 2\n",
        "\n",
        "model.save(\"./GN08_model_plain.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUWhQjCMfYWd"
      },
      "source": [
        "# Part 2: Evaluating the trained model on Gnutella 04\n",
        "\n",
        "Hints:\n",
        "1. Write down the evaluation using the functions and codes in Part 1\n",
        "2. Compute the groundtruth of betweenness centrality using NetworkX could take around 1 hour. Keep your Colab opened and be patient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8bn7__itfjqZ"
      },
      "outputs": [],
      "source": [
        "'''Gnutella 04'''\n",
        "# change the path to your own directory\n",
        "path2 = './data/p2p-Gnutella04.txt'\n",
        "\n",
        "G2 = nx.read_edgelist(path2, comments='#', delimiter=None, create_using=nx.DiGraph,\n",
        "                  nodetype=None, data=True, edgetype=None, encoding='utf-8')\n",
        "\n",
        "#print(nx.info(G2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "deg_lst2 = [val for (node, val) in G2.degree()]\n",
        "nr_nodes2 = G2.number_of_nodes()\n",
        "degree_norm2, degree_rank2 = _normalize_array_by_rank(deg_lst2, nr_nodes2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing betweenness centrality for G2\n"
          ]
        }
      ],
      "source": [
        "print(\"Computing betweenness centrality for G2\")\n",
        "b2 = [v for v in nx.betweenness_centrality(G2).values()]\n",
        "BC_norm_cent2, BC_cent_rank2 = _normalize_array_by_rank(b2, nr_nodes2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-04 19:57:45.815480: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 946299008 exceeds 10% of free system memory.\n"
          ]
        }
      ],
      "source": [
        "mu_all2 = Structure2Vec(G2, nr_nodes2, degree_norm2, embed_size=EMBED_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model = tf.keras.models.load_model(\"./GN08_model_plain.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step\n",
            "Predictions shape: (10876,)\n",
            "Ground truth shape: 10876\n"
          ]
        }
      ],
      "source": [
        "x_test = mu_all2\n",
        "y_pred2 = model.predict(x_test)\n",
        "\n",
        "# Flatten predictions\n",
        "if len(y_pred2.shape) > 1:\n",
        "    y_pred2 = y_pred2.flatten()\n",
        "\n",
        "print(f\"Predictions shape: {y_pred2.shape}\")\n",
        "print(f\"Ground truth shape: {len(BC_norm_cent2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kendall tau correlation on Gnutella 04: -0.4105517022447545\n",
            "P-value: 0.0\n"
          ]
        }
      ],
      "source": [
        "kendall_tau2, p_value2 = scipy.stats.kendalltau(BC_norm_cent2, y_pred2)\n",
        "\n",
        "print(f\"Kendall tau correlation on Gnutella 04: {kendall_tau2}\")\n",
        "print(f\"P-value: {p_value2}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
